# AlexNet FCå±‚è¿‘ä¼¼å¤„ç†å®ç°è¯´æ˜

## ğŸ“‹ ä¿®æ”¹æ¦‚è¿°

ä¸ºAlexNetæ·»åŠ äº†FCå±‚ï¼ˆfc1, fc2ï¼‰çš„INT8è¿‘ä¼¼å¤„ç†ï¼Œä½¿å¾—æ€»å…±æœ‰7å±‚éœ€è¦è¿‘ä¼¼ï¼ˆ5ä¸ªå·ç§¯å±‚ + 2ä¸ªFCå±‚ï¼‰ã€‚

---

## ğŸ”§ å…³é”®ä¿®æ”¹ç‚¹

### 1. **å±‚åˆ—è¡¨æ›´æ–°** (`approx_train.py`)

```python
# åŸæ¥ï¼šåªæœ‰5ä¸ªå·ç§¯å±‚
ALEXNET_CONV_LAYERS = ["conv1", "conv2", "conv3", "conv4", "conv5"]

# ç°åœ¨ï¼š5ä¸ªå·ç§¯å±‚ + 2ä¸ªFCå±‚
ALEXNET_LAYERS = ["conv1", "conv2", "conv3", "conv4", "conv5", "fc1", "fc2"]
ALEXNET_CONV_LAYERS = ALEXNET_LAYERS  # å…¼å®¹æ€§åˆ«å
```

### 2. **æ³¨æ„åŠ›å±‚é…ç½®** (`approx_train_ppo.py`)

```python
# æ··åˆé€‰æ‹©å·ç§¯å±‚å’ŒFCå±‚
attention_layers_default = ['conv3', 'conv4', 'fc1']
```

**é€‰æ‹©ç†ç”±ï¼š**
- `conv3`: ä¸­å±‚å·ç§¯ç‰¹å¾
- `conv4`: é«˜å±‚å·ç§¯ç‰¹å¾  
- `fc1`: å…¨è¿æ¥ç‰¹å¾ï¼ˆå…³é”®ï¼‰

### 3. **è®°å½•ç‚¹é”®ååŒºåˆ†**

ä¸åŒç±»å‹çš„å±‚ä½¿ç”¨ä¸åŒçš„é”®åæ ¼å¼ï¼š

| å±‚ç±»å‹ | è®°å½•ç‚¹é”®åæ ¼å¼ | ç¤ºä¾‹ |
|--------|---------------|------|
| å·ç§¯å±‚ | `block_output.{layer_name}` | `block_output.conv1` |
| FCå±‚ | `classifier.{layer_name}.out` | `classifier.fc1.out` |

### 4. **ç›´æ–¹å›¾æ”¶é›†** (`approx_train_ppo.py`)

```python
if model_type == "alexnet":
    hists = {}
    for lyr in CONV_LAYERS_USED:
        if lyr.startswith('conv'):
            hists[f"block_output.{lyr}"] = torch.zeros(256, dtype=torch.long)
        elif lyr.startswith('fc'):
            hists[f"classifier.{lyr}.out"] = torch.zeros(256, dtype=torch.long)
    
    # æ”¶é›†æ—¶ä¹Ÿè¦åŒºåˆ†
    for lyr in CONV_LAYERS_USED:
        if lyr.startswith('conv'):
            k = f"block_output.{lyr}"
        elif lyr.startswith('fc'):
            k = f"classifier.{lyr}.out"
        
        if k in rec.storage:
            v = rec.storage[k].flatten()
            hists[k] += torch.bincount(v, minlength=256).to(torch.long)
```

### 5. **tmaxè®¡ç®—** (`approx_train_ppo.py`)

```python
for layer_name in CONV_LAYERS_USED:
    # æ ¹æ®å±‚ç±»å‹é€‰æ‹©æ­£ç¡®çš„é”®å
    if layer_name.startswith('conv'):
        hist_key = f"block_output.{layer_name}"
    elif layer_name.startswith('fc'):
        hist_key = f"classifier.{layer_name}.out"
    
    # è®¡ç®—p90 + 10
    p90_code = _q_nonzero_from_hist(hist, 0.9)
    tmax_code = p90_code + 10
```

### 6. **è¿‘ä¼¼ç¼–è¾‘ç‚¹** (`approx_train_ppo.py`)

è®­ç»ƒå¾ªç¯å’Œè¯„ä¼°å‡½æ•°ä¸­éƒ½éœ€è¦æ­£ç¡®è®¾ç½®edité”®ï¼š

```python
# Build edits
edits = {}
for layer_name, action_codes in layer_actions.items():
    s, z = scales.get(layer_name, (1.0, 0.0))
    tmax_code = layer_tmax_codes[layer_name]
    
    # é€‰æ‹©æ­£ç¡®çš„é”®å
    if layer_name.startswith('conv'):
        key = f"block_output.{layer_name}"
    elif layer_name.startswith('fc'):
        key = f"classifier.{layer_name}.out"
    
    def make_edit(codes, s_, z_, tmax_):
        def _fn(x):
            return TriApproxINT8_PPO(x, s_, z_, codes, tmax_)
        return _fn
    
    edits[key] = make_edit(action_codes.to(device), s, z, tmax_code)
```

### 7. **Scaleæ”¶é›†å¢å¼º** (`approx_train.py`)

```python
@torch.no_grad()
def teacher_forward_with_scales(teacher, images_cpu):
    scales: Dict[str, Tuple[float,float]] = {}
    def make_sniffer(name):
        def _fn(x):
            scales[name] = (float(x.q_scale()), float(x.q_zero_point()))
            return x
        return _fn
    
    edits = {}
    # VGG16çš„å·ç§¯å±‚
    for k in CONV_LAYERS:
        edits[f"block_output.{k}"] = make_sniffer(k)
    
    # AlexNetçš„FCå±‚
    if hasattr(teacher, 'fc1'):  # æ£€æµ‹æ˜¯å¦ä¸ºAlexNet
        for fc_name in ['fc1', 'fc2']:
            edits[f"classifier.{fc_name}.out"] = make_sniffer(fc_name)
    
    rec = ActivationRecorder(store_cpu=False, edits=edits)
    logits = teacher(images_cpu, recorder=rec)
    return logits, scales
```

### 8. **Scaleç¼“å­˜å…œåº•** (`approx_train_ppo.py`)

```python
# For AlexNet FC layers, ensure we have scale information
if model_type == "alexnet":
    for lyr in ['fc1', 'fc2']:
        if lyr not in scale_cache:
            scale_cache[lyr] = (1.0, 0.0)  # é»˜è®¤scale
```

---

## ğŸ¯ å·¥ä½œæµç¨‹

### å®Œæ•´çš„æ•°æ®æµ

```
1. è®­ç»ƒå¼€å§‹
   â””â”€ æ¨¡å‹é€‰æ‹©: model_type = "alexnet"
   â””â”€ å±‚åˆ—è¡¨: CONV_LAYERS_USED = ['conv1'...'conv5', 'fc1', 'fc2']

2. ç›´æ–¹å›¾æ”¶é›† (calib_batches=16)
   â”œâ”€ å·ç§¯å±‚: block_output.conv1 â†’ 256ç»´ç›´æ–¹å›¾
   â”œâ”€ å·ç§¯å±‚: block_output.conv2 â†’ 256ç»´ç›´æ–¹å›¾
   â”œâ”€ ...
   â”œâ”€ FCå±‚: classifier.fc1.out â†’ 256ç»´ç›´æ–¹å›¾
   â””â”€ FCå±‚: classifier.fc2.out â†’ 256ç»´ç›´æ–¹å›¾

3. tmaxè®¡ç®—
   â”œâ”€ æ¯å±‚: å–p90åˆ†ä½æ•° + 10
   â”œâ”€ conv1: tmax = 35 (ä¾‹å¦‚)
   â”œâ”€ fc1: tmax = 28 (FCå±‚é€šå¸¸æ›´å°)
   â””â”€ fc2: tmax = 25

4. çŠ¶æ€ç¼–ç å™¨åˆå§‹åŒ–
   â””â”€ 7å±‚ Ã— 4DçŠ¶æ€å‘é‡
       â”œâ”€ [layer_depth_ratio, act_mean, act_std, p90]
       â””â”€ conv1: [0.0, 0.45, 0.23, 0.85]
           fc1: [0.83, 0.38, 0.19, 0.72]

5. PPOè®­ç»ƒå¾ªç¯
   â”œâ”€ Episodeå¼€å§‹
   â”œâ”€ é‡‡æ ·batch
   â”œâ”€ Teacherå‰å‘ (æ”¶é›†scales)
   â”‚   â”œâ”€ conv1 scale: (0.125, 128)
   â”‚   â”œâ”€ fc1 scale: (0.098, 130)
   â”‚   â””â”€ ...
   â”œâ”€ å¯¹æ¯å±‚é‡‡æ ·åŠ¨ä½œ
   â”‚   â”œâ”€ conv1: state â†’ policy â†’ [t1=7, v1=14, t2=21, v2=28]
   â”‚   â”œâ”€ fc1: state â†’ policy â†’ [t1=6, v1=12, t2=18, v2=24]
   â”‚   â””â”€ ...
   â”œâ”€ æ„å»ºeditså­—å…¸
   â”‚   â”œâ”€ "block_output.conv1" â†’ TriApproxINT8_PPO(...)
   â”‚   â”œâ”€ "classifier.fc1.out" â†’ TriApproxINT8_PPO(...)
   â”‚   â””â”€ ...
   â”œâ”€ Studentå‰å‘ (åº”ç”¨è¿‘ä¼¼)
   â”œâ”€ è®¡ç®—KD loss
   â”œâ”€ è®¡ç®—Attention loss (conv3, conv4, fc1)
   â”œâ”€ è®¡ç®—reward
   â””â”€ æ›´æ–°PPO

6. åå¤„ç†
   â”œâ”€ Top-30é…ç½®è·Ÿè¸ª
   â”œâ”€ æ‰¹é‡è¯„ä¼°
   â””â”€ ä¿å­˜ç»“æœ (åŒ…å«7å±‚é…ç½®)
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. **é”®åå¿…é¡»ç²¾ç¡®åŒ¹é…**

âŒ é”™è¯¯ï¼š
```python
# FCå±‚é”™è¯¯ä½¿ç”¨å·ç§¯å±‚æ ¼å¼
key = f"block_output.fc1"  # æ‰¾ä¸åˆ°ï¼
```

âœ… æ­£ç¡®ï¼š
```python
# FCå±‚ä½¿ç”¨æ­£ç¡®æ ¼å¼
key = f"classifier.fc1.out"  # OK
```

### 2. **æ‰€æœ‰å¤„ç†ç‚¹éƒ½è¦æ›´æ–°**

éœ€è¦ä¿®æ”¹çš„ä½ç½®ï¼ˆå…±6å¤„ï¼‰ï¼š
- âœ… ç›´æ–¹å›¾æ”¶é›†
- âœ… tmaxè®¡ç®—
- âœ… editsæ„å»ºï¼ˆè®­ç»ƒå¾ªç¯ï¼‰
- âœ… editsæ„å»ºï¼ˆè¯„ä¼°å‡½æ•°ï¼‰
- âœ… scaleæ”¶é›†å¢å¼º
- âœ… scaleç¼“å­˜å…œåº•

### 3. **FCå±‚ç‰¹æ€§**

| ç‰¹æ€§ | å·ç§¯å±‚ | FCå±‚ |
|------|--------|------|
| **è¾“å…¥å½¢çŠ¶** | [B, C, H, W] | [B, Features] |
| **æ¿€æ´»å€¼èŒƒå›´** | è¾ƒå¤§ | é€šå¸¸è¾ƒå° |
| **tmaxæœŸæœ›** | 30-50 | 20-35 |
| **è¿‘ä¼¼éš¾åº¦** | ä¸­ç­‰ | è¾ƒå®¹æ˜“ |

### 4. **fc3ä¸è¿‘ä¼¼**

```python
# fc3ç›´æ¥è¾“å‡ºlogitsï¼Œä¸è¿›è¡Œè¿‘ä¼¼
# åŸå› ï¼š
# 1. è¾“å‡ºå±‚å¯¹ç²¾åº¦æ•æ„Ÿ
# 2. è¿‘ä¼¼å¯èƒ½å¯¼è‡´åˆ†ç±»è¾¹ç•Œåç§»
# 3. ä¸æ˜¯ReLUæ¿€æ´»ï¼Œè¿‘ä¼¼æ•ˆæœå·®
```

---

## ğŸ§ª æµ‹è¯•éªŒè¯

### æ£€æŸ¥ç‚¹æ¸…å•

```bash
# 1. æ£€æŸ¥å±‚æ•°
# æ—¥å¿—åº”æ˜¾ç¤º: "Using AlexNet with 7 layers (5 conv + 2 fc)"

# 2. æ£€æŸ¥åˆå§‹åŒ–è¾“å‡º
# åº”è¯¥çœ‹åˆ°7å±‚çš„tmaxåˆå§‹åŒ–:
#   [init] conv1: t1=7, v1=14, t2=21, v2=28, tmax=35 (p90+10)
#   ...
#   [init] fc1: t1=6, v1=12, t2=18, v2=24, tmax=30 (p90+10)
#   [init] fc2: t1=5, v1=10, t2=15, v2=20, tmax=25 (p90+10)

# 3. æ£€æŸ¥ç»“æœJSON
# åº”åŒ…å«7ä¸ªå±‚çš„é…ç½®
cat outputs/tri_ppo_int_codes/alexnet_exp1.json | grep -c "tmax_code"
# æœŸæœ›è¾“å‡º: 7

# 4. æ£€æŸ¥æ³¨æ„åŠ›å±‚
# æ—¥å¿—åº”æ˜¾ç¤º: "Attention Transfer: 3 layers"
# å±‚ååº”ä¸º: conv3, conv4, fc1

# 5. éªŒè¯FCå±‚scale
# æ·»åŠ è°ƒè¯•è¾“å‡ºæ£€æŸ¥scale_cacheæ˜¯å¦åŒ…å«fc1å’Œfc2
```

### å¸¸è§é—®é¢˜

**é—®é¢˜1: KeyError: 'classifier.fc1.out'**
```
åŸå› : ç›´æ–¹å›¾æ”¶é›†æ—¶é”®åä¸åŒ¹é…
è§£å†³: ç¡®è®¤ä½¿ç”¨æ­£ç¡®çš„é”®åæ ¼å¼
```

**é—®é¢˜2: FCå±‚tmaxå¼‚å¸¸å¤§/å°**
```
åŸå› : FCå±‚æ¿€æ´»å€¼åˆ†å¸ƒå¯èƒ½ä¸å·ç§¯å±‚ä¸åŒ
è§£å†³: è¿™æ˜¯æ­£å¸¸çš„ï¼ŒFCå±‚é€šå¸¸èŒƒå›´æ›´é›†ä¸­
```

**é—®é¢˜3: scale_cacheä¸­æ²¡æœ‰fc1/fc2**
```
åŸå› : teacher_forward_with_scalesæœªæ”¶é›†FCå±‚scale
è§£å†³: å·²æ·»åŠ å…œåº•é€»è¾‘ï¼Œä½¿ç”¨é»˜è®¤(1.0, 0.0)
```

---

## ğŸ“Š é¢„æœŸæ•ˆæœ

### æ€§èƒ½å¯¹æ¯”

| é…ç½® | å±‚æ•° | Episodeæ—¶é—´ | ç²¾åº¦å½±å“ |
|------|------|------------|---------|
| **åªè¿‘ä¼¼å·ç§¯å±‚** | 5å±‚ | ~å¿« | å° |
| **è¿‘ä¼¼å·ç§¯+FC** | 7å±‚ | ~ä¸­ç­‰ | ç¨å¤§ |

### FCå±‚è¿‘ä¼¼çš„æ„ä¹‰

1. **æ›´å…¨é¢çš„é‡åŒ–**
   - è¦†ç›–æ•´ä¸ªæ¨ç†è·¯å¾„
   - FCå±‚ä¹Ÿæœ‰å¤§é‡è®¡ç®—

2. **ç²¾åº¦æƒè¡¡**
   - FCå±‚æ¿€æ´»å€¼é€šå¸¸æ›´ç¨³å®š
   - è¿‘ä¼¼å½±å“ç›¸å¯¹å¯æ§

3. **æ¢ç´¢ç©ºé—´æ‰©å¤§**
   - çŠ¶æ€ç©ºé—´: 5Ã—4D â†’ 7Ã—4D
   - åŠ¨ä½œç©ºé—´: 5Ã—4ç  â†’ 7Ã—4ç 
   - æœç´¢éš¾åº¦å¢åŠ ï¼Œä½†æ›´ç»†ç²’åº¦

---

## ğŸš€ å®é™…ä½¿ç”¨

å®Œæ•´å‘½ä»¤ä¸å˜ï¼Œè‡ªåŠ¨å¤„ç†FCå±‚ï¼š

```bash
python main.py --mode train_ppo_triseg --model alexnet \
  --backend fbgemm --lr 0.0003 --episodes 10000 \
  --batch-size 32 --calib-batches 16 \
  --result-file alexnet_with_fc.json
```

è¾“å‡ºå°†è‡ªåŠ¨åŒ…å«7å±‚é…ç½®ï¼
